\section{Training}%
\label{sec:train}

\subsection{Loss-Funktion}%
\label{sec:loss}

\begin{frame}{Loss-Funktion/ Cost-Funktion}
  \begin{itemize}
  \item Dient zur Berechnung des Fehlers während dem Training
  \item Trainingsfehler soll minimiert werden
  \item \(\Rightarrow\) wir suchen den Punkt, an dem die Ableitung der
    Loss-Funktion \(0\) wird, der Fehler also nicht mehr abnimmt
  \item Es gibt eine Vielzahl an Loss- oder Cost-Funktionen, wir betrachten hier
    die \enquote{\textbf{Quadratic Cost/ Mean Squared Error (MSE)}}:
  \end{itemize}

  \vspace{.6cm}

  \begin{minipage}{.4\linewidth}
    \[C(w, b) = \frac{1}{2n} \sum_x ||y(x) - a||^2\]
  \end{minipage}\hfill%
  \begin{minipage}{.55\linewidth}
    \uncover<2->{
      \begin{tabular}{ll}
        \toprule
        \(C(w, b)\) & Cost in Abhängigkeit von \(w\) und \(b\)\\
        \(n\) & Anzahl der Trainingsinstanzen\\
        \(y(x)\) & Label wenn \(x\) Input ist\\
        \(a\) & Output des Netzwerkes, bei \(w\) und \(b\)\\
        \bottomrule
      \end{tabular}
    }
  \end{minipage}
\end{frame}

\subsection{Gradient Descent}%
\label{sec:graddesc}

\begin{frame}{Gradient Descent}
  
\end{frame}

\subsection{Backpropagation}%
\label{sec:backprop}

\begin{frame}{Backpropagation}
  Es werden vier Gleichungen benötigt:

  Error im Output-Layer:

  Error einzelner Neuronen:

  \[\delta_j^L = \frac{\partial C}{\partial a_j^L} \sigma' \left(z_j^L\right)\]

  Vektorisiert:

  \[\delta^L = \nabla_aC \odot \sigma'(z^L)\]

  Aufgelöst, wenn MSE benutzt:

  \[\delta^L = (a^L - y) \odot \sigma'(z^L)\]
\end{frame}

\begin{frame}{Backpropagation}
  Error im Layer \(l\) hinsichtlich Error im nächsten Layer \(\delta^{l+1}\)

  \[\delta^l = \left(\left(w^{l+1}\right)^T\delta^{l+1}\right) \odot
    \sigma'\left(z^l\right)\]

  \begin{itemize}
  \item Rekursive Definition durch Verwendung von \(\delta^l\) in Abhängigkeit
    von \(\delta^{l+1}\)
  \item Wenn anfangs \(\delta^L\) in die Gleichung gegeben wird kann der Error
    rekursiv für jeden vorhergehenden Layer berechnet werden
  \end{itemize}
\end{frame}

\begin{frame}{Backpropagation}
  \begin{align*}
    \delta^L &= \nabla_aC \odot \sigma' \left(z^L\right)\\[1em]
    \delta^l &= \left(\left(w^{l+1}\right)^T \delta^{l+1}\right) \odot
               \sigma'\left(z^l\right)\\[1em]
    \frac{\partial C}{\partial b^l_j} &= \delta^l_j\\[1em]
    \frac{\partial C}{\partial w^l_{jk}} &= a^{l-1}_k\delta^l_j
  \end{align*}
\end{frame}

\begin{frame}{Beispiel}
  Pass
\end{frame}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../präsentation"
%%% End:
